{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a real linear model\n",
    "\n",
    "\\begin{equation*}\n",
    "y = wx + b\n",
    "\\end{equation*}\n",
    "\n",
    "For trained parameters $\\hat w$ and $\\hat b$, the model is \n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat y(x)=\\hat w x + \\hat b\n",
    "\\end{equation*}\n",
    "\n",
    "And the Mean Square Error (MSE) loss between $y$ and $\\hat y$ is\n",
    "\n",
    "\\begin{equation*}\n",
    "L = \\sqrt{\\lVert y - \\hat y\\rVert^2}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume a single $x$ is $3$-vector, i.e. $x\\in \\mathbb{R}^3$, $y$ is a $2$-vector, i.e. $y\\in\\mathbb{R}^2$.\n",
    "There are $10$ samples, i.e. $10$ $3$-vectors as input and $10$ $2$-vectors as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1766,  0.1254,  0.2699],\n",
      "        [-1.5509,  1.8803,  0.3243],\n",
      "        [-1.7099, -0.1290, -0.9486],\n",
      "        [-0.8850,  0.3447, -0.8812],\n",
      "        [ 1.2934, -1.2040,  0.2890],\n",
      "        [ 0.1278, -1.0115, -0.1379],\n",
      "        [-0.7354,  0.2185, -0.6483],\n",
      "        [ 0.0492, -2.1378, -0.3556],\n",
      "        [ 0.3415,  1.1722,  1.2577],\n",
      "        [ 0.0516,  0.0733, -0.6183]])\n",
      "tensor([[-0.4190,  1.5812],\n",
      "        [-0.2929, -0.5735],\n",
      "        [ 1.6416,  0.4896],\n",
      "        [ 0.3663, -0.4154],\n",
      "        [-0.9183, -1.6822],\n",
      "        [-1.6864, -0.3989],\n",
      "        [-0.6359, -0.1090],\n",
      "        [-1.9653, -1.1053],\n",
      "        [-0.0496, -0.3079],\n",
      "        [ 0.3461, -0.5509]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.randn(10, 3)\n",
    "y = torch.randn(10, 2)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a linear model, so we build a fully connected (linear) layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:  Parameter containing:\n",
      "tensor([[-0.3412,  0.3215, -0.3616],\n",
      "        [ 0.1967,  0.3356,  0.4529]], requires_grad=True)\n",
      "b:  Parameter containing:\n",
      "tensor([-0.4701, -0.2400], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "linear = nn.Linear(3, 2)\n",
    "print('w: ', linear.weight)\n",
    "print('b: ', linear.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the weight $w$ is a $2\\times3$ matrix, while the bias $b$ is a $2$-vector. The implementation is actually $y = xw^T + b^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function and optimizer are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD means [Stochastic Gradient Descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent). We can simply understand it as a technique to improve parameter $w$ of $y=wx+b$ by backpropagate error of $y$ to $w$:\n",
    "\n",
    "\\begin{equation*}\n",
    "w_{n+1} = w_n -\\eta E_n(w) \n",
    "\\end{equation*}\n",
    "\n",
    "where $E(w)$ is the loss of $y$ relative to $w$, recorded as gradient for each tensor, $\\eta$ is the learning rate (here is $0.01$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we apply the linear model to input data $x$ (Forward pass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5876, -0.0410],\n",
      "        [ 0.5464,  0.2327],\n",
      "        [ 0.4149, -1.0494],\n",
      "        [ 0.2613, -0.6976],\n",
      "        [-1.4030, -0.2587],\n",
      "        [-0.7891, -0.6167],\n",
      "        [ 0.0855, -0.6050],\n",
      "        [-1.0457, -1.1088],\n",
      "        [-0.6645,  0.7902],\n",
      "        [-0.2406, -0.4854]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "pred = linear(x)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss is computed as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.7318679094314575\n"
     ]
    }
   ],
   "source": [
    "loss = criterion(pred, y)\n",
    "print('loss: ', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss should be backpropagated to improve parameters (Backward pass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dL/dw:  tensor([[-0.0379, -0.1217,  0.0014],\n",
      "        [ 0.3901,  0.1111,  0.3637]])\n",
      "dL/db:  tensor([ 0.0191, -0.0767])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print('dL/dw: ', linear.weight.grad)\n",
    "print('dL/db: ', linear.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these gradients ready, we can improve the parameters now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.3409,  0.3227, -0.3616],\n",
      "        [ 0.1928,  0.3344,  0.4493]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.4703, -0.2393], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "optimizer.step()\n",
    "print(linear.weight)\n",
    "print(linear.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or equivalently, we can do it manulay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.3405,  0.3239, -0.3616],\n",
      "        [ 0.1889,  0.3333,  0.4457]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.4705, -0.2385], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "linear.weight.data.sub_(0.01 * linear.weight.grad.data)\n",
    "linear.bias.data.sub_(0.01 * linear.bias.grad.data)\n",
    "print(linear.weight)\n",
    "print(linear.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss after optimization is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is:  0.7255462408065796\n"
     ]
    }
   ],
   "source": [
    "pred = linear(x)\n",
    "loss = criterion(pred, y)\n",
    "print('loss is: ', loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
